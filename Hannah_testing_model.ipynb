{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLqLlsCDNvAf",
        "outputId": "cb6c2e2b-f570-4b79-bc79-aa4a89d31dbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 14.2 kB/114 kB 12%] [Connected to cloud.r\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 113 kB/114 kB 99%] [Connected to cloud.r-\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
            "\r0% [3 InRelease 33.0 kB/114 kB 29%] [1 InRelease 113 kB/114 kB 99%] [Connected \r0% [3 InRelease 43.1 kB/114 kB 38%] [Connected to cloud.r-project.org (108.138.\r                                                                               \rHit:4 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:13 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Fetched 336 kB in 2s (152 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.2  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.2.3'\n",
        "spark_version = 'spark-3.2.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"basictestdata\").getOrCreate()"
      ],
      "metadata": {
        "id": "eiSmYZd2RXYh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "# url =\"https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.1/22-big-data/day_2/yelp_reviews.csv\"\n",
        "spark.sparkContext.addFile(\"Resources/basictestdata.csv\")\n",
        "df = spark.read.csv(SparkFiles.get(\"basictestdata.csv\"), sep=\",\", header=False)\n",
        "\n",
        "# Show DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8ukpXTMRhp7",
        "outputId": "c22ed4f1-c615-41d1-d1e5-9e09391118ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+\n",
            "| _c0|                 _c1|\n",
            "+----+--------------------+\n",
            "|spam|        this is spam|\n",
            "| ham|         this is ham|\n",
            "| ham|         ham is good|\n",
            "|spam|         spam is bad|\n",
            "|spam|most spam is irri...|\n",
            "|spam|some spam is dang...|\n",
            "| ham|         ham is nice|\n",
            "| ham|ham comes from re...|\n",
            "|spam|        no more spam|\n",
            "| ham|       need more ham|\n",
            "|spam|           stop spam|\n",
            "| ham|         send me ham|\n",
            "+----+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import length\n",
        "# Create a length column to be used as a future feature \n",
        "data_df = df.withColumn('length', length(df['_c1']))\n",
        "data_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scwysHdHRxfJ",
        "outputId": "004d7cf2-49dd-4f17-9f18-9cda17da475d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+------+\n",
            "| _c0|                 _c1|length|\n",
            "+----+--------------------+------+\n",
            "|spam|        this is spam|    12|\n",
            "| ham|         this is ham|    11|\n",
            "| ham|         ham is good|    11|\n",
            "|spam|         spam is bad|    11|\n",
            "|spam|most spam is irri...|    23|\n",
            "|spam|some spam is dang...|    22|\n",
            "| ham|         ham is nice|    11|\n",
            "| ham|ham comes from re...|    28|\n",
            "|spam|        no more spam|    12|\n",
            "| ham|       need more ham|    13|\n",
            "|spam|           stop spam|     9|\n",
            "| ham|         send me ham|    11|\n",
            "+----+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer"
      ],
      "metadata": {
        "id": "Uibr2jBaRye1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol=\"_c1\", outputCol=\"words\")\n",
        "tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2-c9w-0SFDc",
        "outputId": "f24c57fa-d34e-48c9-f6fd-05a628e8e85a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Tokenizer_08bb81d452d1"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = tokenizer.transform(data_df)\n",
        "tokenized.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5vbNzQGSUE8",
        "outputId": "691fa29c-8517-4e71-962b-7386f54ed08f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------------------------+------+----------------------------------+\n",
            "|_c0 |_c1                         |length|words                             |\n",
            "+----+----------------------------+------+----------------------------------+\n",
            "|spam|this is spam                |12    |[this, is, spam]                  |\n",
            "|ham |this is ham                 |11    |[this, is, ham]                   |\n",
            "|ham |ham is good                 |11    |[ham, is, good]                   |\n",
            "|spam|spam is bad                 |11    |[spam, is, bad]                   |\n",
            "|spam|most spam is irritating     |23    |[most, spam, is, irritating]      |\n",
            "|spam|some spam is dangerous      |22    |[some, spam, is, dangerous]       |\n",
            "|ham |ham is nice                 |11    |[ham, is, nice]                   |\n",
            "|ham |ham comes from real contacts|28    |[ham, comes, from, real, contacts]|\n",
            "|spam|no more spam                |12    |[no, more, spam]                  |\n",
            "|ham |need more ham               |13    |[need, more, ham]                 |\n",
            "|spam|stop spam                   |9     |[stop, spam]                      |\n",
            "|ham |send me ham                 |11    |[send, me, ham]                   |\n",
            "+----+----------------------------+------+----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")"
      ],
      "metadata": {
        "id": "Q4fK8KcCSnm1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "removed_df = remover.transform(tokenized)\n",
        "removed_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlv8QfDoTJuR",
        "outputId": "e05895fb-8a85-414a-e8b6-ccb1b6e2d35b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+------+--------------------+--------------------+\n",
            "| _c0|                 _c1|length|               words|            filtered|\n",
            "+----+--------------------+------+--------------------+--------------------+\n",
            "|spam|        this is spam|    12|    [this, is, spam]|              [spam]|\n",
            "| ham|         this is ham|    11|     [this, is, ham]|               [ham]|\n",
            "| ham|         ham is good|    11|     [ham, is, good]|         [ham, good]|\n",
            "|spam|         spam is bad|    11|     [spam, is, bad]|         [spam, bad]|\n",
            "|spam|most spam is irri...|    23|[most, spam, is, ...|  [spam, irritating]|\n",
            "|spam|some spam is dang...|    22|[some, spam, is, ...|   [spam, dangerous]|\n",
            "| ham|         ham is nice|    11|     [ham, is, nice]|         [ham, nice]|\n",
            "| ham|ham comes from re...|    28|[ham, comes, from...|[ham, comes, real...|\n",
            "|spam|        no more spam|    12|    [no, more, spam]|              [spam]|\n",
            "| ham|       need more ham|    13|   [need, more, ham]|         [need, ham]|\n",
            "|spam|           stop spam|     9|        [stop, spam]|        [stop, spam]|\n",
            "| ham|         send me ham|    11|     [send, me, ham]|         [send, ham]|\n",
            "+----+--------------------+------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hashing = HashingTF(inputCol=\"filtered\", outputCol=\"hashedValues\", numFeatures=pow(2,4))\n",
        "\n",
        "# Transform into a DF\n",
        "hashed_df = hashing.transform(removed_df)"
      ],
      "metadata": {
        "id": "w9QhLXYlTiGe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hashed_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIqyrtTrTo1d",
        "outputId": "23afb531-c0ff-43b4-fbda-df84d4ea2f0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+------+--------------------+--------------------+--------------------+\n",
            "| _c0|                 _c1|length|               words|            filtered|        hashedValues|\n",
            "+----+--------------------+------+--------------------+--------------------+--------------------+\n",
            "|spam|        this is spam|    12|    [this, is, spam]|              [spam]|      (16,[1],[1.0])|\n",
            "| ham|         this is ham|    11|     [this, is, ham]|               [ham]|     (16,[14],[1.0])|\n",
            "| ham|         ham is good|    11|     [ham, is, good]|         [ham, good]|(16,[8,14],[1.0,1...|\n",
            "|spam|         spam is bad|    11|     [spam, is, bad]|         [spam, bad]|(16,[1,4],[1.0,1.0])|\n",
            "|spam|most spam is irri...|    23|[most, spam, is, ...|  [spam, irritating]|(16,[1,8],[1.0,1.0])|\n",
            "|spam|some spam is dang...|    22|[some, spam, is, ...|   [spam, dangerous]|(16,[1,8],[1.0,1.0])|\n",
            "| ham|         ham is nice|    11|     [ham, is, nice]|         [ham, nice]|(16,[10,14],[1.0,...|\n",
            "| ham|ham comes from re...|    28|[ham, comes, from...|[ham, comes, real...|(16,[1,7,11,14],[...|\n",
            "|spam|        no more spam|    12|    [no, more, spam]|              [spam]|      (16,[1],[1.0])|\n",
            "| ham|       need more ham|    13|   [need, more, ham]|         [need, ham]|(16,[9,14],[1.0,1...|\n",
            "|spam|           stop spam|     9|        [stop, spam]|        [stop, spam]|(16,[1,12],[1.0,1...|\n",
            "| ham|         send me ham|    11|     [send, me, ham]|         [send, ham]|(16,[3,14],[1.0,1...|\n",
            "+----+--------------------+------+--------------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idf = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
        "idfModel = idf.fit(hashed_df)\n",
        "rescaledData = idfModel.transform(hashed_df)"
      ],
      "metadata": {
        "id": "Wqf8zMFHT9vl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rescaledData.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKGMLr4HUAkg",
        "outputId": "c546d86d-40d4-47c4-945f-ff1e6744596d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----------------------------+------+----------------------------------+----------------------------+----------------------------------+----------------------------------------------------------------------------------------------+\n",
            "|_c0 |_c1                         |length|words                             |filtered                    |hashedValues                      |features                                                                                      |\n",
            "+----+----------------------------+------+----------------------------------+----------------------------+----------------------------------+----------------------------------------------------------------------------------------------+\n",
            "|spam|this is spam                |12    |[this, is, spam]                  |[spam]                      |(16,[1],[1.0])                    |(16,[1],[0.4855078157817008])                                                                 |\n",
            "|ham |this is ham                 |11    |[this, is, ham]                   |[ham]                       |(16,[14],[1.0])                   |(16,[14],[0.6190392084062235])                                                                |\n",
            "|ham |ham is good                 |11    |[ham, is, good]                   |[ham, good]                 |(16,[8,14],[1.0,1.0])             |(16,[8,14],[1.1786549963416462,0.6190392084062235])                                           |\n",
            "|spam|spam is bad                 |11    |[spam, is, bad]                   |[spam, bad]                 |(16,[1,4],[1.0,1.0])              |(16,[1,4],[0.4855078157817008,1.8718021769015913])                                            |\n",
            "|spam|most spam is irritating     |23    |[most, spam, is, irritating]      |[spam, irritating]          |(16,[1,8],[1.0,1.0])              |(16,[1,8],[0.4855078157817008,1.1786549963416462])                                            |\n",
            "|spam|some spam is dangerous      |22    |[some, spam, is, dangerous]       |[spam, dangerous]           |(16,[1,8],[1.0,1.0])              |(16,[1,8],[0.4855078157817008,1.1786549963416462])                                            |\n",
            "|ham |ham is nice                 |11    |[ham, is, nice]                   |[ham, nice]                 |(16,[10,14],[1.0,1.0])            |(16,[10,14],[1.8718021769015913,0.6190392084062235])                                          |\n",
            "|ham |ham comes from real contacts|28    |[ham, comes, from, real, contacts]|[ham, comes, real, contacts]|(16,[1,7,11,14],[1.0,1.0,1.0,1.0])|(16,[1,7,11,14],[0.4855078157817008,1.8718021769015913,1.8718021769015913,0.6190392084062235])|\n",
            "|spam|no more spam                |12    |[no, more, spam]                  |[spam]                      |(16,[1],[1.0])                    |(16,[1],[0.4855078157817008])                                                                 |\n",
            "|ham |need more ham               |13    |[need, more, ham]                 |[need, ham]                 |(16,[9,14],[1.0,1.0])             |(16,[9,14],[1.8718021769015913,0.6190392084062235])                                           |\n",
            "|spam|stop spam                   |9     |[stop, spam]                      |[stop, spam]                |(16,[1,12],[1.0,1.0])             |(16,[1,12],[0.4855078157817008,1.8718021769015913])                                           |\n",
            "|ham |send me ham                 |11    |[send, me, ham]                   |[send, ham]                 |(16,[3,14],[1.0,1.0])             |(16,[3,14],[1.8718021769015913,0.6190392084062235])                                           |\n",
            "+----+----------------------------+------+----------------------------------+----------------------------+----------------------------------+----------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indexer = StringIndexer(inputCol='_c0',outputCol='label')"
      ],
      "metadata": {
        "id": "S2M_3uzQUU2L"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexed_df  = indexer.fit(rescaledData).transform(rescaledData)\n",
        "indexed_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqnflVCYUiUN",
        "outputId": "4fe30d5d-3e65-42d6-be0a-8a9255ac1d37"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "| _c0|                 _c1|length|               words|            filtered|        hashedValues|            features|label|\n",
            "+----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "|spam|        this is spam|    12|    [this, is, spam]|              [spam]|      (16,[1],[1.0])|(16,[1],[0.485507...|  1.0|\n",
            "| ham|         this is ham|    11|     [this, is, ham]|               [ham]|     (16,[14],[1.0])|(16,[14],[0.61903...|  0.0|\n",
            "| ham|         ham is good|    11|     [ham, is, good]|         [ham, good]|(16,[8,14],[1.0,1...|(16,[8,14],[1.178...|  0.0|\n",
            "|spam|         spam is bad|    11|     [spam, is, bad]|         [spam, bad]|(16,[1,4],[1.0,1.0])|(16,[1,4],[0.4855...|  1.0|\n",
            "|spam|most spam is irri...|    23|[most, spam, is, ...|  [spam, irritating]|(16,[1,8],[1.0,1.0])|(16,[1,8],[0.4855...|  1.0|\n",
            "|spam|some spam is dang...|    22|[some, spam, is, ...|   [spam, dangerous]|(16,[1,8],[1.0,1.0])|(16,[1,8],[0.4855...|  1.0|\n",
            "| ham|         ham is nice|    11|     [ham, is, nice]|         [ham, nice]|(16,[10,14],[1.0,...|(16,[10,14],[1.87...|  0.0|\n",
            "| ham|ham comes from re...|    28|[ham, comes, from...|[ham, comes, real...|(16,[1,7,11,14],[...|(16,[1,7,11,14],[...|  0.0|\n",
            "|spam|        no more spam|    12|    [no, more, spam]|              [spam]|      (16,[1],[1.0])|(16,[1],[0.485507...|  1.0|\n",
            "| ham|       need more ham|    13|   [need, more, ham]|         [need, ham]|(16,[9,14],[1.0,1...|(16,[9,14],[1.871...|  0.0|\n",
            "|spam|           stop spam|     9|        [stop, spam]|        [stop, spam]|(16,[1,12],[1.0,1...|(16,[1,12],[0.485...|  1.0|\n",
            "| ham|         send me ham|    11|     [send, me, ham]|         [send, ham]|(16,[3,14],[1.0,1...|(16,[3,14],[1.871...|  0.0|\n",
            "+----+--------------------+------+--------------------+--------------------+--------------------+--------------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = indexed_df.drop('label')\n",
        "y = indexed_df.select('label')"
      ],
      "metadata": {
        "id": "9Nrpt2uR6WUT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler "
      ],
      "metadata": {
        "id": "5i89eArS6iPa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "dbXcmMuA6VEr",
        "outputId": "bd25dcf8-51d6-43b4-eea8-9f3497ee2104"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-df9fbe379f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"At least one array required as input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2559\u001b[0;31m     \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2561\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_make_indexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \"\"\"\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \"\"\"\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected sequence or array-like, got <class 'pyspark.sql.dataframe.DataFrame'>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = indexed_df.select(['label','hashedValues','features'])\n",
        "cleaned_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqPOek7hvWYO",
        "outputId": "51f68fd2-04e4-47a4-a15b-e51f1aa4d733"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+\n",
            "|label|        hashedValues|            features|\n",
            "+-----+--------------------+--------------------+\n",
            "|  1.0|      (16,[1],[1.0])|(16,[1],[0.485507...|\n",
            "|  0.0|     (16,[14],[1.0])|(16,[14],[0.61903...|\n",
            "|  0.0|(16,[8,14],[1.0,1...|(16,[8,14],[1.178...|\n",
            "|  1.0|(16,[1,4],[1.0,1.0])|(16,[1,4],[0.4855...|\n",
            "|  1.0|(16,[1,8],[1.0,1.0])|(16,[1,8],[0.4855...|\n",
            "|  1.0|(16,[1,8],[1.0,1.0])|(16,[1,8],[0.4855...|\n",
            "|  0.0|(16,[10,14],[1.0,...|(16,[10,14],[1.87...|\n",
            "|  0.0|(16,[1,7,11,14],[...|(16,[1,7,11,14],[...|\n",
            "|  1.0|      (16,[1],[1.0])|(16,[1],[0.485507...|\n",
            "|  0.0|(16,[9,14],[1.0,1...|(16,[9,14],[1.871...|\n",
            "|  1.0|(16,[1,12],[1.0,1...|(16,[1,12],[0.485...|\n",
            "|  0.0|(16,[3,14],[1.0,1...|(16,[3,14],[1.871...|\n",
            "+-----+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F"
      ],
      "metadata": {
        "id": "WypEocoVrS-i"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = cleaned_df.drop(['labels'])\n",
        "df_sizes = indexed_df.select(*[F.size(c).alias(c) for c in columns])\n",
        "df_max = df_sizes.agg(*[F.max(c).alias(c) for c in columns])\n",
        "max_dict = df_max.collect()[0].asDict()\n",
        "\n",
        "df_result = df.select('label', *[df[c][i] for c in columns for i in range(max_dict[c])])\n",
        "df_result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "ntHRxuA5rVOh",
        "outputId": "9bc9551e-6fbf-4c55-af3e-ea76f54b5249"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-18f13e4faa48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleaned_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sizes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2533\u001b[0m                 \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2534\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2535\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be a string or a Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2536\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2537\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: col should be a string or a Column"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sizes = df2.select(F.size('col2').alias('col2'))\n",
        "df_max = df_sizes.agg(F.max('col2'))\n",
        "nb_columns = df_max.collect()[0][0]\n",
        "\n",
        "df_result = df2.select('col1', *[df2['col2'][i] for i in range(nb_columns)])\n",
        "df_result.show()"
      ],
      "metadata": {
        "id": "cyNFLF-1sARh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "10dAq-IZmLhz",
        "outputId": "4bc43d34-b50b-42d2-dc8d-ddace25e519c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-56-4e5ffe1de1bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexed_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeature_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2478\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'length(features)' due to data type mismatch: argument 1 requires (string or binary) type, however, 'features' is of struct<type:tinyint,size:int,indices:array<int>,values:array<double>> type.;\n'Project [_c0#47, _c1#48, length(features#188) AS length#653, words#84, filtered#114, hashedValues#148, features#188, label#324]\n+- Project [_c0#47, _c1#48, length#62, words#84, filtered#114, hashedValues#148, features#188, UDF(cast(_c0#47 as string)) AS label#324]\n   +- Project [_c0#47, _c1#48, length#62, words#84, filtered#114, hashedValues#148, UDF(hashedValues#148) AS features#188]\n      +- Project [_c0#47, _c1#48, length#62, words#84, filtered#114, UDF(filtered#114) AS hashedValues#148]\n         +- Project [_c0#47, _c1#48, length#62, words#84, UDF(words#84) AS filtered#114]\n            +- Project [_c0#47, _c1#48, length#62, UDF(_c1#48) AS words#84]\n               +- Project [_c0#47, _c1#48, length(_c1#48) AS length#62]\n                  +- Relation [_c0#47,_c1#48] csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_df = indexed_df.select([\"label\",\"features\",\"length\"])\n",
        "clean_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sh6eic9oWiU1",
        "outputId": "105a756b-863b-4b1f-b16d-f20d9f4b906c"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+------+\n",
            "|label|            features|length|\n",
            "+-----+--------------------+------+\n",
            "|  1.0|(16,[1],[0.485507...|    12|\n",
            "|  0.0|(16,[14],[0.61903...|    11|\n",
            "|  0.0|(16,[8,14],[1.178...|    11|\n",
            "|  1.0|(16,[1,4],[0.4855...|    11|\n",
            "|  1.0|(16,[1,8],[0.4855...|    23|\n",
            "|  1.0|(16,[1,8],[0.4855...|    22|\n",
            "|  0.0|(16,[10,14],[1.87...|    11|\n",
            "|  0.0|(16,[1,7,11,14],[...|    28|\n",
            "|  1.0|(16,[1],[0.485507...|    12|\n",
            "|  0.0|(16,[9,14],[1.871...|    13|\n",
            "|  1.0|(16,[1,12],[0.485...|     9|\n",
            "|  0.0|(16,[3,14],[1.871...|    11|\n",
            "+-----+--------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n"
      ],
      "metadata": {
        "id": "OivQoB1n22Q7"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pyspark.sql.functions as sf\n",
        "import pyspark.sql.types as sparktypes\n"
      ],
      "metadata": {
        "id": "K5VsqcPH9yY-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NipL2mJ4941n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_df = clean_df.select(col(\"label\"),col(\"features._1\").alias(\"Tokens\"), col(\"features._2\").alias(\"weights\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "qqBfX9t928MX",
        "outputId": "92b07ed7-bc51-4032-caef-a6aa02c3e92e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-261bfe4b083a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features._1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tokens\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"features._2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1683\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \"\"\"\n\u001b[0;32m-> 1685\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Can't extract value from features#157: need struct type but got struct<type:tinyint,size:int,indices:array<int>,values:array<double>>"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VH7-Saw115SB"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_pd_df = pd.DataFrame(clean_df)\n",
        "clean_pd_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "86vWW3Im1uwv",
        "outputId": "811185b8-3c4b-4a56-aed8-963a4d86830b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-a8ca3d669047>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclean_pd_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mclean_pd_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DataFrame constructor not properly called!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;31m# Argument 1 to \"ensure_index\" has incompatible type \"Collection[Any]\";\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: DataFrame constructor not properly called!"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = clean_df.select('features')\n",
        "features_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmKRW17HlIUa",
        "outputId": "a84b666e-a826-448a-b812-3cdd2754e9de"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|(16,[1],[0.485507...|\n",
            "|(16,[14],[0.61903...|\n",
            "|(16,[8,14],[1.178...|\n",
            "|(16,[1,4],[0.4855...|\n",
            "|(16,[1,8],[0.4855...|\n",
            "|(16,[1,8],[0.4855...|\n",
            "|(16,[10,14],[1.87...|\n",
            "|(16,[1,7,11,14],[...|\n",
            "|(16,[1],[0.485507...|\n",
            "|(16,[9,14],[1.871...|\n",
            "|(16,[1,12],[0.485...|\n",
            "|(16,[3,14],[1.871...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for row in features_df.itertuples(index=False):\n",
        "    print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "ehaqpJXY8ZCm",
        "outputId": "97b8ba62-0dfb-4088-cfc4-a97b588cb84a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-09d96ad86df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1657\u001b[0m         \"\"\"\n\u001b[1;32m   1658\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1659\u001b[0;31m             raise AttributeError(\n\u001b[0m\u001b[1;32m   1660\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[1;32m   1661\u001b[0m         \u001b[0mjc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'itertuples'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = features_df.withColumn('feature_list', list(col('features')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "RvarqoaU0JzM",
        "outputId": "e956f3df-0b35-4232-b3ab-814974350864"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-b98b3a5ed87f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_list'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.3-bin-hadoop3.2/python/pyspark/sql/column.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column is not iterable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;31m# string methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "# Break data down into a training set and a testing set\n",
        "training, testing = cleaned_df.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Create a Naive Bayes model and fit training data\n",
        "model = NaiveBayes()\n",
        "predictor = model.fit(training)"
      ],
      "metadata": {
        "id": "h6A7L_v5XRw9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = predictor.transform(testing)\n",
        "test_results.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XybA0pdIXc9n",
        "outputId": "7a27d86b-da7e-4cbf-9174-283f742e32d6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|label|        hashedValues|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|  1.0|(16,[1,12],[1.0,1...|(16,[1,12],[0.485...|[-8.5018155317944...|[0.27471734930404...|       1.0|\n",
            "+-----+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XSdmPGSXlzK",
        "outputId": "62610108-eb3e-478d-b6b1-c01340b8cde4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model at predicting reviews was: 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"basictestmodel1.h5\")"
      ],
      "metadata": {
        "id": "-U42_8xUXvCD"
      },
      "execution_count": 27,
      "outputs": []
    }
  ]
}